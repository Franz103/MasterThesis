{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==1.3.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Collecting aiohttp==3.8.3\n",
      "  Using cached aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Collecting aiosignal==1.2.0\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting alembic==1.8.1\n",
      "  Using cached alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "Collecting async-timeout==4.0.2\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs==22.1.0\n",
      "  Using cached attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting autopage==0.5.1\n",
      "  Using cached autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting cachetools==5.2.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting certifi==2022.9.24\n",
      "  Using cached certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "Collecting charset-normalizer==2.1.1\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting cliff==4.0.0\n",
      "  Using cached cliff-4.0.0-py3-none-any.whl (80 kB)\n",
      "Collecting cmaes==0.8.2\n",
      "  Using cached cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting cmd2==2.4.2\n",
      "  Using cached cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: colorama==0.4.5 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.4.5)\n",
      "Collecting colorlog==6.7.0\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting contourpy==1.0.5\n",
      "  Using cached contourpy-1.0.5-cp39-cp39-win_amd64.whl (161 kB)\n",
      "Requirement already satisfied: cramjam==2.5.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (2.5.0)\n",
      "Requirement already satisfied: cycler==0.11.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 18)) (0.11.0)\n",
      "Collecting fastparquet==0.8.3\n",
      "  Using cached fastparquet-0.8.3-cp39-cp39-win_amd64.whl (605 kB)\n",
      "Collecting fonttools==4.37.4\n",
      "  Using cached fonttools-4.37.4-py3-none-any.whl (960 kB)\n",
      "Collecting frozenlist==1.3.1\n",
      "  Using cached frozenlist-1.3.1-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Using cached fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Collecting google-auth==2.12.0\n",
      "  Using cached google_auth-2.12.0-py2.py3-none-any.whl (169 kB)\n",
      "Collecting google-auth-oauthlib==0.4.6\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting greenlet==1.1.3.post0\n",
      "  Using cached greenlet-1.1.3.post0-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Collecting grpcio==1.49.1\n",
      "  Using cached grpcio-1.49.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Collecting idna==3.4\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting importlib-metadata==5.0.0\n",
      "  Using cached importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources==5.10.0\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting joblib==1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting kiwisolver==1.4.4\n",
      "  Using cached kiwisolver-1.4.4-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Collecting Mako==1.2.3\n",
      "  Using cached Mako-1.2.3-py3-none-any.whl (78 kB)\n",
      "Collecting Markdown==3.4.1\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting MarkupSafe==2.1.1\n",
      "  Using cached MarkupSafe-2.1.1-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting matplotlib==3.6.1\n",
      "  Using cached matplotlib-3.6.1-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting multidict==6.0.2\n",
      "  Using cached multidict-6.0.2-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: numpy==1.23.4 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 37)) (1.23.4)\n",
      "Requirement already satisfied: oauthlib==3.2.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 38)) (3.2.1)\n",
      "Collecting optuna==2.10.1\n",
      "  Using cached optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "Requirement already satisfied: packaging==21.3 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 40)) (21.3)\n",
      "Collecting pandas==1.5.0\n",
      "  Using cached pandas-1.5.0-cp39-cp39-win_amd64.whl (10.9 MB)\n",
      "Collecting patsy==0.5.3\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: pbr==5.10.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 43)) (5.10.0)\n",
      "Requirement already satisfied: Pillow==9.2.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 44)) (9.2.0)\n",
      "Requirement already satisfied: prettytable==3.4.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 45)) (3.4.1)\n",
      "Requirement already satisfied: protobuf==3.19.6 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 46)) (3.19.6)\n",
      "Collecting pyarrow==9.0.0\n",
      "  Using cached pyarrow-9.0.0-cp39-cp39-win_amd64.whl (19.6 MB)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 48)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 49)) (0.2.8)\n",
      "Requirement already satisfied: pyDeprecate==0.3.2 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 50)) (0.3.2)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 51)) (3.0.9)\n",
      "Requirement already satisfied: pyperclip==1.8.2 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 52)) (1.8.2)\n",
      "Requirement already satisfied: pyreadline3==3.4.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 53)) (3.4.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 54)) (2.8.2)\n",
      "Collecting pytorch-forecasting==0.10.3\n",
      "  Using cached pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "Collecting pytorch-lightning==1.7.7\n",
      "  Using cached pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "Requirement already satisfied: pytz==2022.4 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 57)) (2022.4)\n",
      "Requirement already satisfied: PyYAML==6.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 58)) (6.0)\n",
      "Requirement already satisfied: requests==2.28.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 59)) (2.28.1)\n",
      "Collecting requests-oauthlib==1.3.1\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: rsa==4.9 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 61)) (4.9)\n",
      "Collecting scikit-learn==1.1.2\n",
      "  Using cached scikit_learn-1.1.2-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "Collecting scipy==1.9.2\n",
      "  Using cached scipy-1.9.2-cp39-cp39-win_amd64.whl (40.1 MB)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Collecting SQLAlchemy==1.4.41\n",
      "  Using cached SQLAlchemy-1.4.41-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: statsmodels==0.13.2 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 66)) (0.13.2)\n",
      "Collecting stevedore==4.0.1\n",
      "  Using cached stevedore-4.0.1-py3-none-any.whl (49 kB)\n",
      "Collecting tensorboard==2.10.1\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 69)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 70)) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 71)) (3.1.0)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Collecting torchmetrics==0.10.0\n",
      "  Using cached torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
      "Requirement already satisfied: tqdm==4.64.1 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 74)) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions==4.4.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 75)) (4.4.0)\n",
      "Requirement already satisfied: urllib3==1.26.12 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 76)) (1.26.12)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 77)) (0.2.5)\n",
      "Collecting Werkzeug==2.2.2\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting yarl==1.8.1\n",
      "  Using cached yarl-1.8.1-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Requirement already satisfied: zipp==3.9.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 80)) (3.9.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from pytorch-lightning==1.7.7->-r requirements.txt (line 56)) (2022.7.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from tensorboard==2.10.1->-r requirements.txt (line 68)) (63.4.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\franzschramm\\anaconda3\\lib\\site-packages (from tensorboard==2.10.1->-r requirements.txt (line 68)) (0.37.1)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "  Using cached fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "Installing collected packages: torch, stevedore, scipy, pyarrow, patsy, multidict, MarkupSafe, kiwisolver, joblib, importlib-resources, importlib-metadata, idna, grpcio, greenlet, fsspec, frozenlist, fonttools, contourpy, colorlog, cmaes, charset-normalizer, certifi, cachetools, autopage, attrs, async-timeout, absl-py, yarl, Werkzeug, torchmetrics, SQLAlchemy, scikit-learn, pandas, matplotlib, Markdown, Mako, google-auth, cmd2, aiosignal, requests-oauthlib, fastparquet, cliff, alembic, aiohttp, optuna, google-auth-oauthlib, tensorboard, pytorch-lightning, pytorch-forecasting\n",
      "Successfully installed Mako-1.2.3 Markdown-3.4.1 MarkupSafe-2.1.1 SQLAlchemy-1.4.41 Werkzeug-2.2.2 absl-py-1.3.0 aiohttp-3.8.3 aiosignal-1.2.0 alembic-1.8.1 async-timeout-4.0.2 attrs-22.1.0 autopage-0.5.1 cachetools-5.2.0 certifi-2022.9.24 charset-normalizer-2.1.1 cliff-4.0.0 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 contourpy-1.0.5 fastparquet-0.8.3 fonttools-4.37.4 frozenlist-1.3.1 fsspec-2022.8.2 google-auth-2.12.0 google-auth-oauthlib-0.4.6 greenlet-1.1.3.post0 grpcio-1.49.1 idna-3.4 importlib-metadata-5.0.0 importlib-resources-5.10.0 joblib-1.2.0 kiwisolver-1.4.4 matplotlib-3.6.1 multidict-6.0.2 optuna-2.10.1 pandas-1.5.0 patsy-0.5.3 pyarrow-9.0.0 pytorch-forecasting-0.10.3 pytorch-lightning-1.7.7 requests-oauthlib-1.3.1 scikit-learn-1.1.2 scipy-1.9.2 stevedore-4.0.1 tensorboard-2.10.1 torch-1.12.1 torchmetrics-0.10.0 yarl-1.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script plasma_store.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mako-render.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script optuna.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\FranzSchramm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clyent==1.2.1\n",
      "  Downloading clyent-1.2.1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: clyent\n",
      "  Building wheel for clyent (setup.py): started\n",
      "  Building wheel for clyent (setup.py): finished with status 'done'\n",
      "  Created wheel for clyent: filename=clyent-1.2.1-py3-none-any.whl size=9192 sha256=848faf884d33a3103a86683b4334fc90e627cafda33a3867eaeecbf235b3fd08\n",
      "  Stored in directory: c:\\users\\franzschramm\\appdata\\local\\pip\\cache\\wheels\\36\\00\\a6\\496d95012b21c0d7b1c980147f150246fc122768aaa222491f\n",
      "Successfully built clyent\n",
      "Installing collected packages: clyent\n",
      "  Attempting uninstall: clyent\n",
      "    Found existing installation: clyent 1.2.2\n",
      "    Uninstalling clyent-1.2.2:\n",
      "      Successfully uninstalled clyent-1.2.2\n",
      "Successfully installed clyent-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.7.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install clyent==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2261857832.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\FranzSchramm\\AppData\\Local\\Temp\\ipykernel_17924\\2261857832.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from pytorch-forecasting.data.examples import get_stallion_data\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pytorch-forecasting.data.examples import get_stallion_data\n",
    "import numpy as np\n",
    "data = get_stallion_data()  # load data as pandas dataframe\n",
    "\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "# categories have to be strings\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"sku\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "data[\"avg_volume_by_agency\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"agency\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "# we want to encode special days as one variable and \n",
    "# thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\", \"good_friday\", \"new_year\", \"christmas\",\n",
    "    \"labor_day\", \"independence_day\", \"revolution_day_memorial\",\n",
    "    \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\",\n",
    "    \"beer_capital\", \"music_fest\"\n",
    "]\n",
    "data[special_days] = (\n",
    "    data[special_days]\n",
    "    .apply(lambda x: x.map({0: \"-\", 1: x.name}))\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "max_prediction_length = 6  # forecast 6 months\n",
    "max_encoder_length = 24  # use 24 months of history\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=0,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\n",
    "        \"avg_population_2017\",\n",
    "        \"avg_yearly_household_income_2017\"\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # group of categorical variables can be treated as \n",
    "    # one variable\n",
    "    variable_groups={\"special_days\": special_days},\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        \"price_regular\",\n",
    "        \"discount_in_percent\"\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"]#, coerce_positive=1.0\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    add_relative_time_idx=True,  # add as feature\n",
    "    add_target_scales=True,  # add as feature\n",
    "    add_encoder_length=True,  # add as feature\n",
    ")\n",
    "# create validation set (predict=True) which means to predict the\n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "# create dataloaders for model\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    #LearningRateLogger\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# stop training, when loss metric does not improve on validation set\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "#lr_logger = LearningRateLogger()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # log to tensorboard\n",
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # train on CPU, use gpus = [0] to run on GPU\n",
    "    gradient_clip_val=0.1,\n",
    "    #early_stop_callback=early_stop_callback,\n",
    "    limit_train_batches=30,  # running validation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
    "    #callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "# initialise model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # biggest influence network size\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
    ")\n",
    "tft.size() # 29.6k parameters in model\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m best_tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# calculate mean absolute error on validation set\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m actuals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_tft\u001b[38;5;241m.\u001b[39mpredict(val_dataloader)\n\u001b[0;32m     10\u001b[0m MAE(predictions, actuals)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "import torch\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# calculate mean absolute error on validation set\n",
    "actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "MAE(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
