{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==1.3.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Collecting aiohttp==3.8.3\n",
      "  Using cached aiohttp-3.8.3-cp38-cp38-win_amd64.whl (324 kB)\n",
      "Requirement already satisfied: aiosignal==1.2.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 3)) (1.2.0)\n",
      "Collecting alembic==1.8.1\n",
      "  Using cached alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Collecting attrs==22.1.0\n",
      "  Using cached attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: autopage==0.5.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: cachetools==5.2.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 8)) (5.2.0)\n",
      "Collecting certifi==2022.9.24\n",
      "  Using cached certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "Collecting charset-normalizer==2.1.1\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting cliff==4.0.0\n",
      "  Using cached cliff-4.0.0-py3-none-any.whl (80 kB)\n",
      "Requirement already satisfied: cmaes==0.8.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 12)) (0.8.2)\n",
      "Collecting cmd2==2.4.2\n",
      "  Using cached cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
      "Collecting colorama==0.4.5\n",
      "  Using cached colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Collecting colorlog==6.7.0\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting contourpy==1.0.5\n",
      "  Using cached contourpy-1.0.5-cp38-cp38-win_amd64.whl (164 kB)\n",
      "Requirement already satisfied: cramjam==2.5.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 17)) (2.5.0)\n",
      "Requirement already satisfied: cycler==0.11.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 18)) (0.11.0)\n",
      "Collecting fastparquet==0.8.3\n",
      "  Using cached fastparquet-0.8.3-cp38-cp38-win_amd64.whl (615 kB)\n",
      "Collecting fonttools==4.37.4\n",
      "  Using cached fonttools-4.37.4-py3-none-any.whl (960 kB)\n",
      "Collecting frozenlist==1.3.1\n",
      "  Using cached frozenlist-1.3.1-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Using cached fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Collecting google-auth==2.12.0\n",
      "  Using cached google_auth-2.12.0-py2.py3-none-any.whl (169 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 24)) (0.4.6)\n",
      "Collecting greenlet==1.1.3.post0\n",
      "  Using cached greenlet-1.1.3.post0-cp38-cp38-win_amd64.whl (101 kB)\n",
      "Collecting grpcio==1.49.1\n",
      "  Using cached grpcio-1.49.1-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Collecting idna==3.4\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting importlib-metadata==5.0.0\n",
      "  Using cached importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources==5.10.0\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting joblib==1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting kiwisolver==1.4.4\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Collecting Mako==1.2.3\n",
      "  Using cached Mako-1.2.3-py3-none-any.whl (78 kB)\n",
      "Collecting Markdown==3.4.1\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: MarkupSafe==2.1.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 34)) (2.1.1)\n",
      "Collecting matplotlib==3.6.1\n",
      "  Using cached matplotlib-3.6.1-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Requirement already satisfied: multidict==6.0.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 36)) (6.0.2)\n",
      "Collecting numpy==1.23.4\n",
      "  Using cached numpy-1.23.4-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting oauthlib==3.2.1\n",
      "  Using cached oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: optuna==2.10.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 39)) (2.10.1)\n",
      "Requirement already satisfied: packaging==21.3 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 40)) (21.3)\n",
      "Collecting pandas==1.5.0\n",
      "  Using cached pandas-1.5.0-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting patsy==0.5.3\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Collecting pbr==5.10.0\n",
      "  Using cached pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "Collecting Pillow==9.2.0\n",
      "  Using cached Pillow-9.2.0-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting prettytable==3.4.1\n",
      "  Using cached prettytable-3.4.1-py3-none-any.whl (26 kB)\n",
      "Collecting protobuf==3.19.6\n",
      "  Using cached protobuf-3.19.6-cp38-cp38-win_amd64.whl (896 kB)\n",
      "Collecting pyarrow==9.0.0\n",
      "  Using cached pyarrow-9.0.0-cp38-cp38-win_amd64.whl (19.6 MB)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 48)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 49)) (0.2.8)\n",
      "Requirement already satisfied: pyDeprecate==0.3.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 50)) (0.3.2)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 51)) (3.0.9)\n",
      "Requirement already satisfied: pyperclip==1.8.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 52)) (1.8.2)\n",
      "Requirement already satisfied: pyreadline3==3.4.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 53)) (3.4.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 54)) (2.8.2)\n",
      "Collecting pytorch-forecasting==0.10.3\n",
      "  Using cached pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "Collecting pytorch-lightning==1.7.7\n",
      "  Using cached pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "Collecting pytz==2022.4\n",
      "  Using cached pytz-2022.4-py2.py3-none-any.whl (500 kB)\n",
      "Requirement already satisfied: PyYAML==6.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 58)) (6.0)\n",
      "Collecting requests==2.28.1\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting requests-oauthlib==1.3.1\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa==4.9\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting scikit-learn==1.1.2\n",
      "  Using cached scikit_learn-1.1.2-cp38-cp38-win_amd64.whl (7.3 MB)\n",
      "Collecting scipy==1.9.2\n",
      "  Using cached scipy-1.9.2-cp38-cp38-win_amd64.whl (39.8 MB)\n",
      "Collecting six==1.16.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting SQLAlchemy==1.4.41\n",
      "  Using cached SQLAlchemy-1.4.41-cp38-cp38-win_amd64.whl (1.6 MB)\n",
      "Collecting statsmodels==0.13.2\n",
      "  Using cached statsmodels-0.13.2-cp38-cp38-win_amd64.whl (9.1 MB)\n",
      "Collecting stevedore==4.0.1\n",
      "  Using cached stevedore-4.0.1-py3-none-any.whl (49 kB)\n",
      "Collecting tensorboard==2.10.1\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 69)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 70)) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 71)) (3.1.0)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp38-cp38-win_amd64.whl (161.9 MB)\n",
      "Collecting torchmetrics==0.10.0\n",
      "  Using cached torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
      "Collecting tqdm==4.64.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting typing_extensions==4.4.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting urllib3==1.26.12\n",
      "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in c:\\python38\\lib\\site-packages (from -r requirements.txt (line 77)) (0.2.5)\n",
      "Collecting Werkzeug==2.2.2\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting yarl==1.8.1\n",
      "  Using cached yarl-1.8.1-cp38-cp38-win_amd64.whl (56 kB)\n",
      "Collecting zipp==3.9.0\n",
      "  Using cached zipp-3.9.0-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\python38\\lib\\site-packages (from pytorch-lightning==1.7.7->-r requirements.txt (line 56)) (2022.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\python38\\lib\\site-packages (from tensorboard==2.10.1->-r requirements.txt (line 68)) (41.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python38\\lib\\site-packages (from tensorboard==2.10.1->-r requirements.txt (line 68)) (0.37.1)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     ------------------------------------ 139.5/139.5 kB 516.9 kB/s eta 0:00:00\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "     ------------------------------------ 138.8/138.8 kB 586.9 kB/s eta 0:00:00\n",
      "Installing collected packages: pytz, zipp, Werkzeug, urllib3, typing_extensions, six, rsa, protobuf, prettytable, Pillow, pbr, oauthlib, numpy, Mako, kiwisolver, joblib, idna, greenlet, fsspec, frozenlist, fonttools, colorama, charset-normalizer, certifi, attrs, absl-py, yarl, tqdm, torch, stevedore, SQLAlchemy, scipy, requests, pyarrow, patsy, importlib-resources, importlib-metadata, grpcio, google-auth, contourpy, colorlog, cmd2, torchmetrics, scikit-learn, requests-oauthlib, pandas, matplotlib, Markdown, cliff, alembic, aiohttp, statsmodels, fastparquet, tensorboard, pytorch-lightning, pytorch-forecasting\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2020.1\n",
      "    Uninstalling pytz-2020.1:\n",
      "      Successfully uninstalled pytz-2020.1\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.8.0\n",
      "    Uninstalling zipp-3.8.0:\n",
      "      Successfully uninstalled zipp-3.8.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.8\n",
      "    Uninstalling rsa-4.8:\n",
      "      Successfully uninstalled rsa-4.8\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "  Attempting uninstall: prettytable\n",
      "    Found existing installation: prettytable 3.3.0\n",
      "    Uninstalling prettytable-3.3.0:\n",
      "      Successfully uninstalled prettytable-3.3.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.1.1\n",
      "    Uninstalling Pillow-9.1.1:\n",
      "      Successfully uninstalled Pillow-9.1.1\n",
      "  Attempting uninstall: pbr\n",
      "    Found existing installation: pbr 5.9.0\n",
      "    Uninstalling pbr-5.9.0:\n",
      "      Successfully uninstalled pbr-5.9.0\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.1.0\n",
      "    Uninstalling oauthlib-3.1.0:\n",
      "      Successfully uninstalled oauthlib-3.1.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: Mako\n",
      "    Found existing installation: Mako 1.2.0\n",
      "    Uninstalling Mako-1.2.0:\n",
      "      Successfully uninstalled Mako-1.2.0\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.3\n",
      "    Uninstalling kiwisolver-1.4.3:\n",
      "      Successfully uninstalled kiwisolver-1.4.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 1.1.2\n",
      "    Uninstalling greenlet-1.1.2:\n",
      "      Successfully uninstalled greenlet-1.1.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.1.0\n",
      "    Uninstalling fsspec-2022.1.0:\n",
      "      Successfully uninstalled fsspec-2022.1.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.3.0\n",
      "    Uninstalling frozenlist-1.3.0:\n",
      "      Successfully uninstalled frozenlist-1.3.0\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.33.3\n",
      "    Uninstalling fonttools-4.33.3:\n",
      "      Successfully uninstalled fonttools-4.33.3\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.3\n",
      "    Uninstalling colorama-0.4.3:\n",
      "      Successfully uninstalled colorama-0.4.3\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.12\n",
      "    Uninstalling charset-normalizer-2.0.12:\n",
      "      Successfully uninstalled charset-normalizer-2.0.12\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.4.5.1\n",
      "    Uninstalling certifi-2020.4.5.1:\n",
      "      Successfully uninstalled certifi-2020.4.5.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.1.0\n",
      "    Uninstalling absl-py-1.1.0:\n",
      "      Successfully uninstalled absl-py-1.1.0\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.7.2\n",
      "    Uninstalling yarl-1.7.2:\n",
      "      Successfully uninstalled yarl-1.7.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0\n",
      "    Uninstalling torch-1.11.0:\n",
      "      Successfully uninstalled torch-1.11.0\n",
      "  Attempting uninstall: stevedore\n",
      "    Found existing installation: stevedore 3.5.0\n",
      "    Uninstalling stevedore-3.5.0:\n",
      "      Successfully uninstalled stevedore-3.5.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.37\n",
      "    Uninstalling SQLAlchemy-1.4.37:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.37\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.8.1\n",
      "    Uninstalling scipy-1.8.1:\n",
      "      Successfully uninstalled scipy-1.8.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 8.0.0\n",
      "    Uninstalling pyarrow-8.0.0:\n",
      "      Successfully uninstalled pyarrow-8.0.0\n",
      "  Attempting uninstall: patsy\n",
      "    Found existing installation: patsy 0.5.1\n",
      "    Uninstalling patsy-0.5.1:\n",
      "      Successfully uninstalled patsy-0.5.1\n",
      "  Attempting uninstall: importlib-resources\n",
      "    Found existing installation: importlib-resources 5.7.1\n",
      "    Uninstalling importlib-resources-5.7.1:\n",
      "      Successfully uninstalled importlib-resources-5.7.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.4\n",
      "    Uninstalling importlib-metadata-4.11.4:\n",
      "      Successfully uninstalled importlib-metadata-4.11.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.46.3\n",
      "    Uninstalling grpcio-1.46.3:\n",
      "      Successfully uninstalled grpcio-1.46.3\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.8.0\n",
      "    Uninstalling google-auth-2.8.0:\n",
      "      Successfully uninstalled google-auth-2.8.0\n",
      "  Attempting uninstall: colorlog\n",
      "    Found existing installation: colorlog 6.6.0\n",
      "    Uninstalling colorlog-6.6.0:\n",
      "      Successfully uninstalled colorlog-6.6.0\n",
      "  Attempting uninstall: cmd2\n",
      "    Found existing installation: cmd2 2.4.1\n",
      "    Uninstalling cmd2-2.4.1:\n",
      "      Successfully uninstalled cmd2-2.4.1\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 0.9.1\n",
      "    Uninstalling torchmetrics-0.9.1:\n",
      "      Successfully uninstalled torchmetrics-0.9.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.1\n",
      "    Uninstalling scikit-learn-1.1.1:\n",
      "      Successfully uninstalled scikit-learn-1.1.1\n",
      "  Attempting uninstall: requests-oauthlib\n",
      "    Found existing installation: requests-oauthlib 1.3.0\n",
      "    Uninstalling requests-oauthlib-1.3.0:\n",
      "      Successfully uninstalled requests-oauthlib-1.3.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.3.7\n",
      "    Uninstalling Markdown-3.3.7:\n",
      "      Successfully uninstalled Markdown-3.3.7\n",
      "  Attempting uninstall: cliff\n",
      "    Found existing installation: cliff 3.10.1\n",
      "    Uninstalling cliff-3.10.1:\n",
      "      Successfully uninstalled cliff-3.10.1\n",
      "  Attempting uninstall: alembic\n",
      "    Found existing installation: alembic 1.8.0\n",
      "    Uninstalling alembic-1.8.0:\n",
      "      Successfully uninstalled alembic-1.8.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.1\n",
      "    Uninstalling aiohttp-3.8.1:\n",
      "      Successfully uninstalled aiohttp-3.8.1\n",
      "  Attempting uninstall: statsmodels\n",
      "    Found existing installation: statsmodels 0.11.1\n",
      "    Uninstalling statsmodels-0.11.1:\n",
      "      Successfully uninstalled statsmodels-0.11.1\n",
      "  Attempting uninstall: fastparquet\n",
      "    Found existing installation: fastparquet 0.8.1\n",
      "    Uninstalling fastparquet-0.8.1:\n",
      "      Successfully uninstalled fastparquet-0.8.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.2.6\n",
      "    Uninstalling pytorch-lightning-1.2.6:\n",
      "      Successfully uninstalled pytorch-lightning-1.2.6\n",
      "  Attempting uninstall: pytorch-forecasting\n",
      "    Found existing installation: pytorch-forecasting 0.10.2\n",
      "    Uninstalling pytorch-forecasting-0.10.2:\n",
      "      Successfully uninstalled pytorch-forecasting-0.10.2\n",
      "Successfully installed Mako-1.2.3 Markdown-3.4.1 Pillow-9.2.0 SQLAlchemy-1.4.41 Werkzeug-2.2.2 absl-py-1.3.0 aiohttp-3.8.3 alembic-1.8.1 attrs-22.1.0 certifi-2022.9.24 charset-normalizer-2.1.1 cliff-4.0.0 cmd2-2.4.2 colorama-0.4.5 colorlog-6.7.0 contourpy-1.0.5 fastparquet-0.8.3 fonttools-4.37.4 frozenlist-1.3.1 fsspec-2022.8.2 google-auth-2.12.0 greenlet-1.1.3.post0 grpcio-1.49.1 idna-3.4 importlib-metadata-5.0.0 importlib-resources-5.10.0 joblib-1.2.0 kiwisolver-1.4.4 matplotlib-3.6.1 numpy-1.23.4 oauthlib-3.2.1 pandas-1.5.0 patsy-0.5.3 pbr-5.10.0 prettytable-3.4.1 protobuf-3.19.6 pyarrow-9.0.0 pytorch-forecasting-0.10.3 pytorch-lightning-1.7.7 pytz-2022.4 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.1.2 scipy-1.9.2 six-1.16.0 statsmodels-0.13.2 stevedore-4.0.1 tensorboard-2.10.1 torch-1.12.1 torchmetrics-0.10.0 tqdm-4.64.1 typing_extensions-4.4.0 urllib3-1.26.12 yarl-1.8.1 zipp-3.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\n",
      "s3fs 2022.1.0 requires fsspec==2022.01.0, but you have fsspec 2022.8.2 which is incompatible.\n",
      "lightning 20220621 requires fsspec==2022.01.0, but you have fsspec 2022.8.2 which is incompatible.\n",
      "lightning 20220621 requires torch<=1.11.0,>=1.9.*, but you have torch 1.12.1 which is incompatible.\n",
      "gcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "3  | prescalers                         | ModuleDict                      | 256   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "29.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.7 K    Total params\n",
      "0.119     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Franz Schramm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1892: PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432df02edd4e4aa6ad4e3a2df09a0b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "import numpy as np\n",
    "data = get_stallion_data()  # load data as pandas dataframe\n",
    "\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "# categories have to be strings\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"sku\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "data[\"avg_volume_by_agency\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"agency\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "# we want to encode special days as one variable and \n",
    "# thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\", \"good_friday\", \"new_year\", \"christmas\",\n",
    "    \"labor_day\", \"independence_day\", \"revolution_day_memorial\",\n",
    "    \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\",\n",
    "    \"beer_capital\", \"music_fest\"\n",
    "]\n",
    "data[special_days] = (\n",
    "    data[special_days]\n",
    "    .apply(lambda x: x.map({0: \"-\", 1: x.name}))\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)\n",
    "\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "max_prediction_length = 6  # forecast 6 months\n",
    "max_encoder_length = 24  # use 24 months of history\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=0,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\n",
    "        \"avg_population_2017\",\n",
    "        \"avg_yearly_household_income_2017\"\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # group of categorical variables can be treated as \n",
    "    # one variable\n",
    "    variable_groups={\"special_days\": special_days},\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        \"price_regular\",\n",
    "        \"discount_in_percent\"\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"]#, coerce_positive=1.0\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    add_relative_time_idx=True,  # add as feature\n",
    "    add_target_scales=True,  # add as feature\n",
    "    add_encoder_length=True,  # add as feature\n",
    ")\n",
    "# create validation set (predict=True) which means to predict the\n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "# create dataloaders for model\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    #LearningRateLogger\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# stop training, when loss metric does not improve on validation set\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "#lr_logger = LearningRateLogger()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # log to tensorboard\n",
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # train on CPU, use gpus = [0] to run on GPU\n",
    "    gradient_clip_val=0.1,\n",
    "    #early_stop_callback=early_stop_callback,\n",
    "    limit_train_batches=30,  # running validation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
    "    #callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "# initialise model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # biggest influence network size\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
    ")\n",
    "tft.size() # 29.6k parameters in model\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m best_tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# calculate mean absolute error on validation set\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m actuals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_tft\u001b[38;5;241m.\u001b[39mpredict(val_dataloader)\n\u001b[0;32m     10\u001b[0m MAE(predictions, actuals)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "import torch\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# calculate mean absolute error on validation set\n",
    "actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "MAE(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
